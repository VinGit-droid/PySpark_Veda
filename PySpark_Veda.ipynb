{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1097cec3",
   "metadata": {},
   "source": [
    "# New PySpark Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9eeafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6331c",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-row-using-rdd-dataframe-2/?expand_article=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e5760",
   "metadata": {},
   "source": [
    "## Create a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f9f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/04 17:48:39 WARN Utils: Your hostname, myThinkPad resolves to a loopback address: 127.0.1.1; using 192.168.1.107 instead (on interface wlp3s0)\n",
      "23/07/04 17:48:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/04 17:48:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .appName(\"Example-3_6\")\n",
    "                     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4b0fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|   Vin|\n",
      "|  2|Shweta|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Create Raw Data\n",
    "\n",
    "data = [(1,\"Vin\"), (2,\"Shweta\")]\n",
    "schema = [\"id\", \"name\"]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4633e22e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Properties, functions and methods of any object(spark in this example)\n",
    "#dir(spark)\n",
    "#help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "da4292a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+---+------+\n",
      "| id|  Name|\n",
      "+---+------+\n",
      "|  1|   Vin|\n",
      "|  2|Shweta|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#schema = StructType(StructField)\n",
    "\n",
    "schema = StructType([StructField(\"id\", IntegerType()), \n",
    "                     StructField(\"Name\", StringType())])\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c97d9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(StructType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437809a",
   "metadata": {},
   "source": [
    "# Read csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a743dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Method 1--------\n",
      "+---+-----------------------------+----------+-----------+----+---+---+\n",
      "|faa|name                         |lat       |lon        |alt |tz |dst|\n",
      "+---+-----------------------------+----------+-----------+----+---+---+\n",
      "|04G|Lansdowne Airport            |41.1304722|-80.6195833|1044|-5 |A  |\n",
      "|06A|Moton Field Municipal Airport|32.4605722|-85.6800278|264 |-5 |A  |\n",
      "|06C|Schaumburg Regional          |41.9893408|-88.1012428|801 |-6 |A  |\n",
      "|06N|Randall Airport              |41.431912 |-74.3915611|523 |-5 |A  |\n",
      "|09J|Jekyll Island Airport        |31.0744722|-81.4277778|11  |-4 |A  |\n",
      "+---+-----------------------------+----------+-----------+----+---+---+\n",
      "\n",
      "-----Method 2--------\n",
      "+-------+----+-----------------------+----------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|type                   |manufacturer    |model   |engines|seats|speed|engine   |\n",
      "+-------+----+-----------------------+----------------+--------+-------+-----+-----+---------+\n",
      "|N102UW |1998|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N103US |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N104UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N105UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N107US |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "+-------+----+-----------------------+----------------+--------+-------+-----+-----+---------+\n",
      "\n",
      "-----Method 3--------\n",
      "+--------------+--------------------+-----+\n",
      "|        author|               title|pages|\n",
      "+--------------+--------------------+-----+\n",
      "| George Orwell|                1984|  245|\n",
      "| Napolean Hill|   Think N Grow Rich|  345|\n",
      "| George Orwell|                1984|  245|\n",
      "| Napolean Hill|   Think N Grow Rich|  345|\n",
      "| Stephen Covey|7 habits of highl...|  500|\n",
      "|Charles Duhigg|  The Power of Habit|  300|\n",
      "|Charles Duhigg|  The Power of Habit|  300|\n",
      "| Eckhart Tolle|    The Power of Now|  600|\n",
      "| Eckhart Tolle|    The Power of Now|  600|\n",
      "| Eckhart Tolle|    The Power of Now|  600|\n",
      "+--------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Don't change this file path\n",
    "file_path = \"rawdata/airports.csv\"\n",
    "\n",
    "# Read in the airports data\n",
    "airports = spark.read.csv(file_path, header=True)\n",
    "print('-----Method 1--------')\n",
    "airports.limit(5).show(truncate=False)\n",
    "\n",
    "#Method 2\n",
    "airports = spark.read.format(\"csv\").load(\"rawdata/planes.csv\", header=True)\n",
    "print('-----Method 2--------')\n",
    "airports.limit(5).show(truncate=False)\n",
    "\n",
    "#Method3 Read list of csv inside a folder\n",
    "airports = spark.read.csv(path=[\"rawdata/books/part-00001-39e30f06-99da-46e9-8d8c-52b65a889529-c000.csv\",\\\n",
    "                                \"rawdata/books/part-00002-39e30f06-99da-46e9-8d8c-52b65a889529-c000.csv\",\\\n",
    "                                \"rawdata/books/part-00003-39e30f06-99da-46e9-8d8c-52b65a889529-c000.csv\",\\\n",
    "                                \"rawdata/books/part-00000-39e30f06-99da-46e9-8d8c-52b65a889529-c000.csv\",\\\n",
    "                                \"rawdata/books/part-00002-39e30f06-99da-46e9-8d8c-52b65a889529-c000.csv\",\\\n",
    "                                \"rawdata/books/part-00003-39e30f06-99da-46e9-8d8c-52b65a889529-c000.csv\",\\\n",
    "                                \"rawdata/books/part-00000-39e30f06-99da-46e9-8d8c-52b65a889529-c000.csv\",\\\n",
    "                                \"rawdata/books/part-00002-39e30f06-99da-46e9-8d8c-52b65a889529-c000.csv\"],\\\n",
    "                          header=True)\n",
    "\n",
    "\n",
    "#df = spark.read.csv(\"path1,path2,path3\")\n",
    "\n",
    "# Show the data\n",
    "print('-----Method 3--------')\n",
    "airports.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2686586",
   "metadata": {},
   "source": [
    "## Write into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37c3624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Charles Duhigg', 'The Power of Habit', 300],\n",
    "        ['Stephen Covey', '7 habits of highly effective people', 500],\n",
    "        ['Eckhart Tolle', 'The Power of Now', 600],\n",
    "        ['George Orwell', '1984', 245],\n",
    "        ['Napolean Hill', 'Think N Grow Rich', 345]]\n",
    "schema = \"author STRING, title STRING, pages INT\"\n",
    "books_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame; it should reflect our table above\n",
    "#books_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "#print(books_df.printSchema())\n",
    "\n",
    "#Save the dataframe\n",
    "\n",
    "#This will save csv in prtitions(Doesn't overwrite)\n",
    "#books_df.write.csv(path=\"rawdata/books\", header=True)\n",
    "\n",
    "books_df.write.mode('overwrite').csv(\"rawdata/books_df\")\n",
    "#you can also use this\n",
    "books_df.write.format(\"csv\").mode('overwrite').save(\"rawdata/books_df\")\n",
    "\n",
    "#Method for single partition\n",
    "#-------\n",
    "\n",
    "books_df.coalesce(1).write.csv(\"rawdata/books_df/Single_Part\")\n",
    "books_df.repartition(1).write.mode('overwrite').csv(\"rawdata/books_df/Single_Part\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e41e9a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----+\n",
      "|        author|               title|pages|\n",
      "+--------------+--------------------+-----+\n",
      "| George Orwell|                1984|  245|\n",
      "| Napolean Hill|   Think N Grow Rich|  345|\n",
      "| Stephen Covey|7 habits of highl...|  500|\n",
      "|Charles Duhigg|  The Power of Habit|  300|\n",
      "| Eckhart Tolle|    The Power of Now|  600|\n",
      "+--------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"rawdata/books\",header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66a4e6",
   "metadata": {},
   "source": [
    "## Read and Write json and parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a415006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|  country|birthdate|   salary|              title|comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|2016-02-03 13:25:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|   Internal Auditor|   1E+02|\n",
      "|2016-02-03 22:34:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|      Accountant IV|        |\n",
      "|2016-02-03 06:39:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|   Russia| 2/1/1960|144972.51|Structural Engineer|        |\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|  country|birthdate|   salary|              title|comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|2016-02-03 13:25:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|   Internal Auditor|   1E+02|\n",
      "|2016-02-03 22:34:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|      Accountant IV|        |\n",
      "|2016-02-03 06:39:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|   Russia| 2/1/1960|144972.51|Structural Engineer|        |\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|  country|birthdate|   salary|              title|comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|2016-02-03 13:25:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|   Internal Auditor|   1E+02|\n",
      "|2016-02-03 22:34:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|      Accountant IV|        |\n",
      "|2016-02-03 06:39:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|   Russia| 2/1/1960|144972.51|Structural Engineer|        |\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Don't change this file path\n",
    "file_path = \"rawdata/userdata1.parquet\"\n",
    "\n",
    "# Read in the raw parquet data\n",
    "\n",
    "df1 = spark.read.parquet(file_path)\n",
    "#df1 = spark.read.option(\"multiLine\", \"true\").option(\"mode\", \"PERMISSIVE\").json(file_path)\n",
    "df1.limit(3).show()\n",
    "\n",
    "#Method 2\n",
    "df2 = spark.read.format(\"parquet\").load(file_path)\n",
    "df2.limit(3).show()\n",
    "\n",
    "#Method3 Read list of csv inside a folder\n",
    "\n",
    "df = spark.read.load([\"rawdata/userdata1.parquet\", \"rawdata/userdata2.parquet\"],format='parquet')\n",
    "\n",
    "# Show the data\n",
    "\n",
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0351cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|  country|birthdate|   salary|              title|comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|2016-02-03 13:25:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|   Internal Auditor|   1E+02|\n",
      "|2016-02-03 22:34:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|      Accountant IV|        |\n",
      "|2016-02-03 06:39:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|   Russia| 2/1/1960|144972.51|Structural Engineer|        |\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|  country|birthdate|   salary|              title|comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|2016-02-03 13:25:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|   Internal Auditor|   1E+02|\n",
      "|2016-02-03 22:34:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|      Accountant IV|        |\n",
      "|2016-02-03 06:39:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|   Russia| 2/1/1960|144972.51|Structural Engineer|        |\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|  Lat|            Location|        LocationText|  LocationType|  Long|RecordNumber|State|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|     US|        false|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE|-66.22|           2|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE|-66.26|          10|   PR|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|        PARC PARQUE|     US|        false|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE|-66.22|           1|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Don't change this file path\n",
    "file_path = \"rawdata/zipcodes.json\"\n",
    "\n",
    "# Read in the raw json data\n",
    "\n",
    "df1 = spark.read.json(file_path)\n",
    "#df1 = spark.read.option(\"multiLine\", \"true\").option(\"mode\", \"PERMISSIVE\").json(file_path)\n",
    "df.limit(3).show()\n",
    "\n",
    "#Method 2\n",
    "df2 = spark.read.format(\"json\").load(file_path)\n",
    "df.limit(3).show()\n",
    "\n",
    "#Method3 Read list of csv inside a folder\n",
    "\n",
    "df = spark.read.json(path=[\"rawdata/zipcode1.json\", \"rawdata/zipcode2.json\"])\n",
    "\n",
    "\n",
    "# Show the data\n",
    "#airports.show()\n",
    "df.limit(3).show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf6918fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|  Lat|            Location|        LocationText|  LocationType|  Long|RecordNumber|State|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|     US|        false|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE|-66.22|           2|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE|-66.26|          10|   PR|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|        PARC PARQUE|     US|        false|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE|-66.22|           1|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.mode('overwrite').json(path=\"rawdata/json_write\")\n",
    "df.write.mode('overwrite').parquet(path=\"rawdata/parquet_write\")\n",
    "df.limit(7).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1648ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96b9474e",
   "metadata": {},
   "source": [
    "## Usage of withColumn() and withColumnRenamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4221b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(df.withColumn)\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "schema = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d329fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|James    |          |Smith   |1991-04-01|M     |300000|\n",
      "|Michael  |Rose      |        |2000-05-19|M     |400000|\n",
      "|Robert   |          |Williams|1978-09-05|M     |400000|\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |400000|\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-100  |\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- CopiedColumn: long (nullable = true)\n",
      "\n",
      "------------Add New Column--------\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- Country: string (nullable = false)\n",
      "\n",
      "------------Add New Column twice--------\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- Country: string (nullable = false)\n",
      " |-- anotherColumn: string (nullable = false)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|anotherColumn|\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA| anotherValue|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA| anotherValue|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA| anotherValue|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA| anotherValue|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA| anotherValue|\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "\n",
      "+---------+----------+--------+----------+---+------+\n",
      "|firstname|middlename|lastname|dob       |sex|salary|\n",
      "+---------+----------+--------+----------+---+------+\n",
      "|James    |          |Smith   |1991-04-01|M  |3000  |\n",
      "|Michael  |Rose      |        |2000-05-19|M  |4000  |\n",
      "|Robert   |          |Williams|1978-09-05|M  |4000  |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F  |4000  |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F  |-1    |\n",
      "+---------+----------+--------+----------+---+------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "#1. Change DataType using PySpark withColumn()\n",
    "\n",
    "df2 = df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\"))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#2. Update The Value of an Existing Column\n",
    "\n",
    "df3 = df.withColumn(\"salary\",col(\"salary\")*100)\n",
    "df3.printSchema()\n",
    "df3.show(truncate=False) \n",
    "\n",
    "#3. Create a Column from an Existing\n",
    "\n",
    "df4 = df.withColumn(\"CopiedColumn\",col(\"salary\")* -1)\n",
    "df4.printSchema()\n",
    "\n",
    "#4. Add a New Column using withColumn()\n",
    "print('------------Add New Column--------')\n",
    "df5 = df.withColumn(\"Country\", lit(\"USA\"))\n",
    "df5.show()\n",
    "df5.printSchema()\n",
    "print('------------Add New Column twice--------')\n",
    "\n",
    "df6 = df.withColumn(\"Country\", lit(\"USA\")).withColumn(\"anotherColumn\",lit(\"anotherValue\"))\n",
    "df6.printSchema()\n",
    "df6.show()\n",
    "\n",
    "#5. Rename Column Name\n",
    "df.withColumnRenamed(\"gender\",\"sex\").show(truncate=False) \n",
    "  \n",
    "#6. Drop Column From PySpark DataFrame\n",
    "df4.drop(\"CopiedColumn\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2dc773",
   "metadata": {},
   "source": [
    "## Usage of StructType() and StructField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "655a0dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+---+------+------+\n",
      "|firstname|middlename|lastname|age|gender|salary|\n",
      "+---------+----------+--------+---+------+------+\n",
      "|James    |          |Smith   |36 |M     |3000  |\n",
      "|Michael  |Rose      |        |40 |M     |4000  |\n",
      "|Robert   |          |Williams|42 |M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39 |F     |4000  |\n",
      "|Jen      |Mary      |Brown   |   |F     |-1    |\n",
      "+---------+----------+--------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"firstname\",StringType(),True), \n",
    "    StructField(\"middlename\",StringType(),True), \n",
    "    StructField(\"lastname\",StringType(),True), \n",
    "    StructField(\"age\", StringType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"salary\", IntegerType(), True) \n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1290c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c73b916",
   "metadata": {},
   "source": [
    "## ArrayType column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb7457c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languagesAtWork: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      " |-- previousState: string (nullable = true)\n",
      "\n",
      "+---------------+------------------+---------------+------------+-------------+\n",
      "|           name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+---------------+------------------+---------------+------------+-------------+\n",
      "|    James,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|  Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|Robert,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+---------------+------------------+---------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"James,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972eea22",
   "metadata": {},
   "source": [
    "## explode(), split() and arrayContains() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d009001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.withColumn()\\ndf.withColumn()\\ndf.withColumn()\\ndf.withColumn()'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df.withColumn()\n",
    "df.withColumn()\n",
    "df.withColumn()\n",
    "df.withColumn()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eec6f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------->  EXPLODE\n",
      "+---------------+------+\n",
      "|           name|   col|\n",
      "+---------------+------+\n",
      "|    James,Smith|  Java|\n",
      "|    James,Smith| Scala|\n",
      "|    James,Smith|   C++|\n",
      "|  Michael,Rose,| Spark|\n",
      "|  Michael,Rose,|  Java|\n",
      "|  Michael,Rose,|   C++|\n",
      "|Robert,Williams|CSharp|\n",
      "|Robert,Williams|    VB|\n",
      "+---------------+------+\n",
      "\n",
      "------------------------------------------->  SPLIT\n",
      "+------------------+\n",
      "|       nameAsArray|\n",
      "+------------------+\n",
      "|    [James, Smith]|\n",
      "| [Michael, Rose, ]|\n",
      "|[Robert, Williams]|\n",
      "+------------------+\n",
      "\n",
      "------------------------------------------->  ALIAS\n",
      "+---------------+--------+\n",
      "|           name|  States|\n",
      "+---------------+--------+\n",
      "|    James,Smith|[OH, CA]|\n",
      "|  Michael,Rose,|[NY, NJ]|\n",
      "|Robert,Williams|[UT, NV]|\n",
      "+---------------+--------+\n",
      "\n",
      "------------------------------------------->  ARRAY CONTAINS\n",
      "+---------------+--------------+\n",
      "|           name|array_contains|\n",
      "+---------------+--------------+\n",
      "|    James,Smith|          true|\n",
      "|  Michael,Rose,|          true|\n",
      "|Robert,Williams|         false|\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode, split, array, array_contains\n",
    "print('------------------------------------------->  EXPLODE')\n",
    "df.select(df.name,explode(df.languagesAtSchool)).show()\n",
    "\n",
    "print('------------------------------------------->  SPLIT')\n",
    "df.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n",
    "\n",
    "print('------------------------------------------->  ARRAY')\n",
    "df.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n",
    "\n",
    "print('------------------------------------------->  ARRAY CONTAINS')\n",
    "df.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n",
    "    .alias(\"array_contains\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa064e",
   "metadata": {},
   "source": [
    "## MapType column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77045fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.withColumn()\n",
    "#df.withColumn()\n",
    "#df.withColumn()\n",
    "#df.withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "724ddab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "+-----+---------+------+--------+\n",
      "|id   |dept     |salary|location|\n",
      "+-----+---------+------+--------+\n",
      "|36636|Finance  |3000  |USA     |\n",
      "|40288|Finance  |5000  |IND     |\n",
      "|42114|Sales    |3900  |USA     |\n",
      "|39192|Marketing|2500  |CAN     |\n",
      "|34534|Sales    |6500  |USA     |\n",
      "+-----+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [ (\"36636\",\"Finance\",3000,\"USA\"), \n",
    "    (\"40288\",\"Finance\",5000,\"IND\"), \n",
    "    (\"42114\",\"Sales\",3900,\"USA\"), \n",
    "    (\"39192\",\"Marketing\",2500,\"CAN\"), \n",
    "    (\"34534\",\"Sales\",6500,\"USA\") ]\n",
    "schema = StructType([\n",
    "     StructField('id', StringType(), True),\n",
    "     StructField('dept', StringType(), True),\n",
    "     StructField('salary', IntegerType(), True),\n",
    "     StructField('location', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "385e1025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- propertiesMap: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+-----+---------+---------------------------------+\n",
      "|id   |dept     |propertiesMap                    |\n",
      "+-----+---------+---------------------------------+\n",
      "|36636|Finance  |{salary -> 3000, location -> USA}|\n",
      "|40288|Finance  |{salary -> 5000, location -> IND}|\n",
      "|42114|Sales    |{salary -> 3900, location -> USA}|\n",
      "|39192|Marketing|{salary -> 2500, location -> CAN}|\n",
      "|34534|Sales    |{salary -> 6500, location -> USA}|\n",
      "+-----+---------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Convert columns to Map\n",
    "from pyspark.sql.functions import col,lit,create_map\n",
    "df = df.withColumn(\"propertiesMap\",create_map(\n",
    "        lit(\"salary\"),col(\"salary\"),\n",
    "        lit(\"location\"),col(\"location\")\n",
    "        )).drop(\"salary\",\"location\")\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88df30",
   "metadata": {},
   "source": [
    "## Row and Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d6ee2",
   "metadata": {},
   "source": [
    "### ------------------Row-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2bbe57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,40\n",
      "Alice\n",
      "James,Alice\n",
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n",
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n",
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n",
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- lang: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/04 01:05:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|            name|              lang|state|\n",
      "+----------------+------------------+-----+\n",
      "|    James,,Smith|[Java, Scala, C++]|   CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|   NV|\n",
      "+----------------+------------------+-----+\n",
      "\n",
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n",
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "row=Row(\"James\",40)\n",
    "print(row[0] +\",\"+str(row[1]))\n",
    "row2=Row(name=\"Alice\", age=11)\n",
    "print(row2.name)\n",
    "\n",
    "Person = Row(\"name\", \"age\")\n",
    "p1=Person(\"James\", 40)\n",
    "p2=Person(\"Alice\", 35)\n",
    "print(p1.name +\",\"+p2.name)\n",
    "\n",
    "#PySpark Example\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "    Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "    Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "\n",
    "#RDD Example 1\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "collData=rdd.collect()\n",
    "print(collData)\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))\n",
    "\n",
    "# RDD Example 2\n",
    "Person=Row(\"name\",\"lang\",\"state\")\n",
    "data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "collData=rdd.collect()\n",
    "print(collData)\n",
    "for person in collData:\n",
    "    print(person.name + \",\" +str(person.lang))\n",
    "\n",
    "#DataFrame Example 1\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "collData=df.collect()\n",
    "print(collData)\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))\n",
    "    \n",
    "#DataFrame Example 2\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df=spark.createDataFrame(data).toDF(*columns)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a737d",
   "metadata": {},
   "source": [
    "### ------------------------Column-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d04147c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name.fname: string (nullable = true)\n",
      " |-- gender: long (nullable = true)\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "+----------+\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import lit\n",
    "colObj = lit(\"sparkbyexamples.com\")\n",
    "\n",
    "\n",
    "\n",
    "data=[(\"James\",23),(\"Ann\",40)]\n",
    "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# Using DataFrame object (df)\n",
    "df.select(df.gender).show()\n",
    "df.select(df[\"gender\"]).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(df[\"`name.fname`\"]).show()\n",
    "\n",
    "#Using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"gender\")).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(col(\"`name.fname`\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048fc41",
   "metadata": {},
   "source": [
    "### ------------------Column Operations------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90c3665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col1 + col2)|\n",
      "+-------------+\n",
      "|          102|\n",
      "|          203|\n",
      "|          304|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 - col2)|\n",
      "+-------------+\n",
      "|           98|\n",
      "|          197|\n",
      "|          296|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 * col2)|\n",
      "+-------------+\n",
      "|          200|\n",
      "|          600|\n",
      "|         1200|\n",
      "+-------------+\n",
      "\n",
      "+-----------------+\n",
      "|    (col1 / col2)|\n",
      "+-----------------+\n",
      "|             50.0|\n",
      "|66.66666666666667|\n",
      "|             75.0|\n",
      "+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 % col2)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            2|\n",
      "|            0|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 > col3)|\n",
      "+-------------+\n",
      "|         true|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 < col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|         true|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 = col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|         true|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark Column Operators\n",
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "\n",
    "#Arthmetic operations\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show() \n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3daa7c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e0e3e1",
   "metadata": {},
   "source": [
    "# alias(), asc(), desc(), cast() and like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1194888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column Functions Examples for alias, ascx, desc, cast, like, \n",
    "\n",
    "data=[(\"James\",\"Bond\",\"100\",None),\n",
    "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
    "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "      (\"Tom Brand\",None,\"400\",'M')] \n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "df=spark.createDataFrame(data,columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca56d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     James|     Bond|\n",
      "|       Ann|    Varsa|\n",
      "|Tom Cruise|      XXX|\n",
      "| Tom Brand|     null|\n",
      "+----------+---------+\n",
      "\n",
      "+--------------+\n",
      "|      fullName|\n",
      "+--------------+\n",
      "|    James,Bond|\n",
      "|     Ann,Varsa|\n",
      "|Tom Cruise,XXX|\n",
      "|          null|\n",
      "+--------------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|       Ann|Varsa|200|     F|\n",
      "|     James| Bond|100|  null|\n",
      "| Tom Brand| null|400|     M|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| null|400|     M|\n",
      "|     James| Bond|100|  null|\n",
      "|       Ann|Varsa|200|     F|\n",
      "+----------+-----+---+------+\n",
      "\n",
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|James| Bond|100|  null|\n",
      "|  Ann|Varsa|200|     F|\n",
      "+-----+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| null|400|     M|\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+---------+-----+---+------+\n",
      "|    fname|lname| id|gender|\n",
      "+---------+-----+---+------+\n",
      "|Tom Brand| null|400|     M|\n",
      "+---------+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|     James| Bond|100|  null|\n",
      "|       Ann|Varsa|200|     F|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+------+\n",
      "|substr|\n",
      "+------+\n",
      "|    Ja|\n",
      "|    An|\n",
      "|    To|\n",
      "|    To|\n",
      "+------+\n",
      "\n",
      "+----------+-----+----------+\n",
      "|     fname|lname|new_gender|\n",
      "+----------+-----+----------+\n",
      "|     James| Bond|      null|\n",
      "|       Ann|Varsa|    Female|\n",
      "|Tom Cruise|  XXX|          |\n",
      "| Tom Brand| null|      Male|\n",
      "+----------+-----+----------+\n",
      "\n",
      "+-----+-----+---+\n",
      "|fname|lname| id|\n",
      "+-----+-----+---+\n",
      "|James| Bond|100|\n",
      "|  Ann|Varsa|200|\n",
      "+-----+-----+---+\n",
      "\n",
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- fname: string (nullable = true)\n",
      " |    |-- lname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------------+\n",
      "|properties[hair]|\n",
      "+----------------+\n",
      "|           black|\n",
      "|           brown|\n",
      "|             red|\n",
      "|           black|\n",
      "+----------------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "|Tom Cruise|\n",
      "| Tom Brand|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#alias\n",
    "from pyspark.sql.functions import expr\n",
    "df.select(df.fname.alias(\"first_name\"), \\\n",
    "          df.lname.alias(\"last_name\")\n",
    "   ).show()\n",
    "\n",
    "#Another example\n",
    "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n",
    "   ).show()\n",
    "\n",
    "\n",
    "#asc, desc to sort ascending and descending order repsectively.\n",
    "df.sort(df.fname.asc()).show()\n",
    "df.sort(df.fname.desc()).show()\n",
    "\n",
    "\n",
    "#cast\n",
    "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n",
    "\n",
    "\n",
    "#between\n",
    "df.filter(df.id.between(100,300)).show()\n",
    "\n",
    "\n",
    "#contains\n",
    "df.filter(df.fname.contains(\"Cruise\")).show()\n",
    "\n",
    "\n",
    "#startswith, endswith()\n",
    "df.filter(df.fname.startswith(\"T\")).show()\n",
    "df.filter(df.fname.endswith(\"Cruise\")).show()\n",
    "\n",
    "\n",
    "#isNull & isNotNull\n",
    "df.filter(df.lname.isNull()).show()\n",
    "df.filter(df.lname.isNotNull()).show()\n",
    "\n",
    "\n",
    "#like , rlike\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.fname.like(\"%om\")) \n",
    "\n",
    "#substr()\n",
    "df.select(df.fname.substr(1,2).alias(\"substr\")).show()\n",
    "\n",
    "\n",
    "#when & otherwise\n",
    "from pyspark.sql.functions import when\n",
    "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
    "              .when(df.gender==\"F\",\"Female\") \\\n",
    "              .when(df.gender==None ,\"\") \\\n",
    "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
    "    ).show()\n",
    "\n",
    "\n",
    "#isin\n",
    "li=[\"100\",\"200\"]\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.id.isin(li)) \\\n",
    "  .show()\n",
    "\n",
    "\n",
    "\n",
    "#Create DataFrame with struct, array & map\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
    "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
    "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
    "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
    "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "            StructField('fname', StringType(), True),\n",
    "            StructField('lname', StringType(), True)])),\n",
    "        StructField('languages', ArrayType(StringType()),True),\n",
    "        StructField('properties', MapType(StringType(),StringType()),True)\n",
    "     ])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "#getField from MapType\n",
    "df.select(df.properties.getField(\"hair\")).show()\n",
    "\n",
    "#getField from Struct\n",
    "df.select(df.name.getField(\"fname\")).show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1f751c6",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-column-functions/?expand_article=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3242c",
   "metadata": {},
   "source": [
    "## filter() and where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6fa510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/04 18:51:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "+----------------+------------------+-----+------+\n",
      "|name            |languages         |state|gender|\n",
      "+----------------+------------------+-----+------+\n",
      "|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |\n",
      "+----------------+------------------+-----+------+\n",
      "\n",
      "+----------------------+------------+-----+------+\n",
      "|name                  |languages   |state|gender|\n",
      "+----------------------+------------+-----+------+\n",
      "|{Julia, , Williams}   |[CSharp, VB]|OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]|OH   |M     |\n",
      "+----------------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col,array_contains\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "arrayStructureData = [\n",
    "        ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    "        ]\n",
    "        \n",
    "arrayStructureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('languages', ArrayType(StringType()), True),\n",
    "         StructField('state', StringType(), True),\n",
    "         StructField('gender', StringType(), True)\n",
    "         ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data = arrayStructureData, schema = arrayStructureSchema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.filter(df.state == \"OH\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "df.filter(col(\"state\") == \"OH\") \\\n",
    "    .show(truncate=False)    \n",
    "    \n",
    "df.filter(\"gender  == 'M'\") \\\n",
    "    .show(truncate=False)    \n",
    "\n",
    "df.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n",
    "    .show(truncate=False)        \n",
    "\n",
    "df.filter(array_contains(df.languages,\"Java\")) \\\n",
    "    .show(truncate=False)        \n",
    "\n",
    "df.filter(df.name.lastname == \"Williams\") \\\n",
    "    .show(truncate=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9069a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using where()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c2b32",
   "metadata": {},
   "source": [
    "## distinct() and dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a87cd6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "Distinct count: 9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Michael      |Sales     |4600  |\n",
      "|James        |Sales     |3000  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "Distinct count: 9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Michael      |Sales     |4600  |\n",
      "|James        |Sales     |3000  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "Distinct count of department salary : 8\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Michael      |Sales     |4600  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import pySpark\n",
    "from pyspark.sql.functions import expr\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "#Distinct\n",
    "distinctDF = df.distinct()\n",
    "print(\"Distinct count: \"+str(distinctDF.count()))\n",
    "distinctDF.show(truncate=False)\n",
    "\n",
    "#Drop duplicates\n",
    "df2 = df.dropDuplicates()\n",
    "print(\"Distinct count: \"+str(df2.count()))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#Drop duplicates on selected columns\n",
    "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
    "print(\"Distinct count of department salary : \"+str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66729ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_name: string, department: string, salary: bigint]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d68f4b",
   "metadata": {},
   "source": [
    "## orderBy() and  sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1504dda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, asc,desc\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.sort(\"department\",\"state\").show(truncate=False)\n",
    "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n",
    "\n",
    "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
    "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n",
    "\n",
    "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
    "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
    "df.orderBy(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
    "\n",
    "df.sort(df.department.asc(),df.state.desc()).show(truncate=False)\n",
    "df.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
    "df.orderBy(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
    "\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e16dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c27cfab",
   "metadata": {},
   "source": [
    "## union() and unionAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0d0a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 218:===================================>                     (5 + 3) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)\n",
    "\n",
    "unionAllDF = df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8acf630",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.union() missing 1 required positional argument: 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrame.union() missing 1 required positional argument: 'other'"
     ]
    }
   ],
   "source": [
    "df.union()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63a39f",
   "metadata": {},
   "source": [
    "## groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03e27a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 229:============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|Sales     |3    |\n",
      "|Finance   |4    |\n",
      "|Marketing |2    |\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|Sales     |NY   |176000     |30000     |\n",
      "|Sales     |CA   |81000      |23000     |\n",
      "|Finance   |CA   |189000     |47000     |\n",
      "|Finance   |NY   |162000     |34000     |\n",
      "|Marketing |NY   |91000      |21000     |\n",
      "|Marketing |CA   |80000      |18000     |\n",
      "+----------+-----+-----------+----------+\n",
      "\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,sum,avg,max\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n",
    "\n",
    "df.groupBy(\"department\").count().show(truncate=False)\n",
    "\n",
    "\n",
    "df.groupBy(\"department\",\"state\") \\\n",
    "    .sum(\"salary\",\"bonus\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)\n",
    "    \n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3aa07a",
   "metadata": {},
   "source": [
    "## groupBy aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff85329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "Example 1 - PySpark groupby agg\n",
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|Sales     |3    |\n",
      "|Finance   |4    |\n",
      "|Marketing |2    |\n",
      "+----------+-----+\n",
      "\n",
      "Example 2 - groupby multiple columns & agg\n",
      "+----------+-----+-----+\n",
      "|department|state|count|\n",
      "+----------+-----+-----+\n",
      "|Sales     |NY   |2    |\n",
      "|Sales     |CA   |1    |\n",
      "|Finance   |CA   |2    |\n",
      "|Finance   |NY   |2    |\n",
      "|Marketing |NY   |1    |\n",
      "|Marketing |CA   |1    |\n",
      "+----------+-----+-----+\n",
      "\n",
      "Example 3 - Multiple Aggregates\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n",
      "Example 4 - Using where on Aggregates\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n",
      "Example 5 - SQL group by agg\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|       avg_salary|sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|     Sales|    257000|85666.66666666667|    53000|    23000|\n",
      "|   Finance|    351000|          87750.0|    81000|    24000|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').master(\"local[5]\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import sum,avg,max,count\n",
    "\n",
    "# Example 1 - PySpark groupby agg\n",
    "print(\"Example 1 - PySpark groupby agg\")\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")\n",
    "     ) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# Example 2 - groupby multiple columns & agg\n",
    "print(\"Example 2 - groupby multiple columns & agg\")\n",
    "df.groupBy(\"department\",\"state\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")\n",
    "     ) \\\n",
    "    .show(truncate=False)\n",
    "    \n",
    "# Example 3 - Multiple Aggregates\n",
    "print(\"Example 3 - Multiple Aggregates\")\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# Example 4 - Using where on Aggregates\n",
    "print(\"Example 4 - Using where on Aggregates\")\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# Example 5 - SQL group by agg\n",
    "# Create Temporary table in PySpark\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "print(\"Example 5 - SQL group by agg\")\n",
    "# PySpark SQL\n",
    "sql_str=\"select department, sum(salary) as sum_salary,\" \\\n",
    "\"avg(salary) as avg_salary,\" \\\n",
    "\"sum(bonus) as sum_bonus,\" \\\n",
    "\"max(bonus) as max_bonus\" \\\n",
    "\" from EMP \"  \\\n",
    "\" group by department having sum_bonus >= 50000\"\n",
    "spark.sql(sql_str).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf51baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "Using unionByName()\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  James| 34|\n",
      "|Michael| 56|\n",
      "| Robert| 30|\n",
      "|  Maria| 24|\n",
      "|  James| 34|\n",
      "|  Maria| 45|\n",
      "|    Jen| 45|\n",
      "|   Jeff| 34|\n",
      "+-------+---+\n",
      "\n",
      "Using allowMissingColumns\n",
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|   5|   2|   6|null|\n",
      "|null|   6|   7|   3|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Create DataFrame df1 with columns name, and id\n",
    "data = [(\"James\",34), (\"Michael\",56), \\\n",
    "        (\"Robert\",30), (\"Maria\",24) ]\n",
    "\n",
    "df1 = spark.createDataFrame(data = data, schema=[\"name\",\"id\"])\n",
    "df1.printSchema()\n",
    "\n",
    "# Create DataFrame df2 with columns name and id\n",
    "data2=[(34,\"James\"),(45,\"Maria\"), \\\n",
    "       (45,\"Jen\"),(34,\"Jeff\")]\n",
    "\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
    "df2.printSchema()\n",
    "\n",
    "# Using unionByName()\n",
    "print(\"Using unionByName()\")\n",
    "df3 = df1.unionByName(df2)\n",
    "#df3.printSchema()\n",
    "df3.show()\n",
    "\n",
    "# Using allowMissingColumns\n",
    "print(\"Using allowMissingColumns\")\n",
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "#df3.printSchema()\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de779a2",
   "metadata": {},
   "source": [
    "## select function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5baa9066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "Select one Column\n",
      "+---------+\n",
      "|firstname|\n",
      "+---------+\n",
      "|    James|\n",
      "|  Michael|\n",
      "|   Robert|\n",
      "|    Maria|\n",
      "+---------+\n",
      "\n",
      "Select multiple Column\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "Using Dataframe object name\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "Using col function\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+-----+------+\n",
      "|name                  |state|gender|\n",
      "+----------------------+-----+------+\n",
      "|{James, null, Smith}  |OH   |M     |\n",
      "|{Anna, Rose, }        |NY   |F     |\n",
      "|{Julia, , Williams}   |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |NY   |M     |\n",
      "|{Mike, Mary, Williams}|OH   |M     |\n",
      "+----------------------+-----+------+\n",
      "\n",
      "+----------------------+\n",
      "|name                  |\n",
      "+----------------------+\n",
      "|{James, null, Smith}  |\n",
      "|{Anna, Rose, }        |\n",
      "|{Julia, , Williams}   |\n",
      "|{Maria, Anne, Jones}  |\n",
      "|{Jen, Mary, Brown}    |\n",
      "|{Mike, Mary, Williams}|\n",
      "+----------------------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|James    |Smith   |\n",
      "|Anna     |        |\n",
      "|Julia    |Williams|\n",
      "|Maria    |Jones   |\n",
      "|Jen      |Brown   |\n",
      "|Mike     |Williams|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "|James    |null      |Smith   |\n",
      "|Anna     |Rose      |        |\n",
      "|Julia    |          |Williams|\n",
      "|Maria    |Anne      |Jones   |\n",
      "|Jen      |Mary      |Brown   |\n",
      "|Mike     |Mary      |Williams|\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"Select one Column\")\n",
    "df.select(\"firstname\").show()\n",
    "\n",
    "print(\"Select multiple Column\")\n",
    "df.select(\"firstname\",\"lastname\").show()\n",
    "\n",
    "#Using Dataframe object name\n",
    "print(\"Using Dataframe object name\")\n",
    "df.select(df.firstname,df.lastname).show()\n",
    "\n",
    "# Using col function\n",
    "from pyspark.sql.functions import col\n",
    "print(\"Using col function\")\n",
    "df.select(col(\"firstname\"),col(\"lastname\")).show()\n",
    "\n",
    "data = [((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType        \n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df2 = spark.createDataFrame(data = data, schema = schema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False) # shows all columns\n",
    "\n",
    "df2.select(\"name\").show(truncate=False)\n",
    "df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)\n",
    "df2.select(\"name.*\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57805f06",
   "metadata": {},
   "source": [
    "# JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2299b286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMP DataFrame\n",
      "+------+--------+-----------+\n",
      "|emp_id|    name|emp_dept_id|\n",
      "+------+--------+-----------+\n",
      "|     1|   Smith|         10|\n",
      "|     2|    Rose|         20|\n",
      "|     3|Williams|         10|\n",
      "|     4|   Jones|         30|\n",
      "+------+--------+-----------+\n",
      "\n",
      "DEPT DataFrame\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n",
      "ADDRESS DataFrame\n",
      "+------+---------------+--------+-----+\n",
      "|emp_id|       addline1|    city|state|\n",
      "+------+---------------+--------+-----+\n",
      "|     1|   1523 Main St|     SFO|   CA|\n",
      "|     2| 3453 Orange St|     SFO|   NY|\n",
      "|     3|   34 Warner St|  Jersey|   NJ|\n",
      "|     4|221 Cavalier St|  Newark|   DE|\n",
      "|     5|  789 Walnut St|Sandiago|   CA|\n",
      "+------+---------------+--------+-----+\n",
      "\n",
      "Join two DataFrames- ADD & EMP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 369:==========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|emp_id|       addline1|  city|state|\n",
      "+------+--------+-----------+------+---------------+------+-----+\n",
      "|     1|   Smith|         10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     2|    Rose|         20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     3|Williams|         10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     4|   Jones|         30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+------+---------------+------+-----+\n",
      "\n",
      "Drop duplicate column\n",
      "+------+--------+-----------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|       addline1|  city|state|\n",
      "+------+--------+-----------+---------------+------+-----+\n",
      "|     1|   Smith|         10|   1523 Main St|   SFO|   CA|\n",
      "|     2|    Rose|         20| 3453 Orange St|   SFO|   NY|\n",
      "|     3|Williams|         10|   34 Warner St|Jersey|   NJ|\n",
      "|     4|   Jones|         30|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+---------------+------+-----+\n",
      "\n",
      "Join Multiple DataFrames-  DEPT, ADD & EMP \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------------+------+-----+---------+-------+\n",
      "|emp_id|    name|emp_dept_id|       addline1|  city|state|dept_name|dept_id|\n",
      "+------+--------+-----------+---------------+------+-----+---------+-------+\n",
      "|     3|Williams|         10|   34 Warner St|Jersey|   NJ|  Finance|     10|\n",
      "|     1|   Smith|         10|   1523 Main St|   SFO|   CA|  Finance|     10|\n",
      "|     2|    Rose|         20| 3453 Orange St|   SFO|   NY|Marketing|     20|\n",
      "|     4|   Jones|         30|221 Cavalier St|Newark|   DE|    Sales|     30|\n",
      "+------+--------+-----------+---------------+------+-----+---------+-------+\n",
      "\n",
      "Using Where for Join Condition- DEPT, ADD & EMP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 388:>                                                        (0 + 4) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|emp_id|       addline1|  city|state|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|     1|   Smith|         10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     2|    Rose|         20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     3|Williams|         10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     4|   Jones|         30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "\n",
      "SQL JOIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 395:========>        (2 + 2) / 4][Stage 396:>                (0 + 2) / 4]\r",
      "\r",
      "[Stage 396:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|emp_id|       addline1|  city|state|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|     1|   Smith|         10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     2|    Rose|         20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     3|Williams|         10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     4|   Jones|         30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName('SparkByExamples.com') \\\n",
    "          .getOrCreate()\n",
    "#EMP DataFrame\n",
    "print(\"EMP DataFrame\")\n",
    "empData = [(1,\"Smith\",10), (2,\"Rose\",20),\n",
    "    (3,\"Williams\",10), (4,\"Jones\",30)\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"emp_dept_id\"]\n",
    "empDF = spark.createDataFrame(empData,empColumns)\n",
    "empDF.show()\n",
    "\n",
    "#DEPT DataFrame\n",
    "print(\"DEPT DataFrame\")\n",
    "deptData = [(\"Finance\",10), (\"Marketing\",20),\n",
    "    (\"Sales\",30),(\"IT\",40)\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF=spark.createDataFrame(deptData,deptColumns)  \n",
    "deptDF.show()\n",
    "\n",
    "#Address DataFrame\n",
    "print(\"ADDRESS DataFrame\")\n",
    "addData=[(1,\"1523 Main St\",\"SFO\",\"CA\"),\n",
    "    (2,\"3453 Orange St\",\"SFO\",\"NY\"),\n",
    "    (3,\"34 Warner St\",\"Jersey\",\"NJ\"),\n",
    "    (4,\"221 Cavalier St\",\"Newark\",\"DE\"),\n",
    "    (5,\"789 Walnut St\",\"Sandiago\",\"CA\")\n",
    "  ]\n",
    "addColumns = [\"emp_id\",\"addline1\",\"city\",\"state\"]\n",
    "addDF = spark.createDataFrame(addData,addColumns)\n",
    "addDF.show()\n",
    "\n",
    "#Join two DataFrames\n",
    "print(\"Join two DataFrames- ADD & EMP\")\n",
    "empDF.join(addDF,empDF[\"emp_id\"] == addDF[\"emp_id\"]).show()\n",
    "\n",
    "#Drop duplicate column\n",
    "print(\"Drop duplicate column\")\n",
    "empDF.join(addDF,[\"emp_id\"]).show()\n",
    "\n",
    "#Join Multiple DataFrames\n",
    "print(\"Join Multiple DataFrames-  DEPT, ADD & EMP \")\n",
    "empDF.join(addDF,[\"emp_id\"]) \\\n",
    "     .join(deptDF,empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
    "     .show()\n",
    "\n",
    "#Using Where for Join Condition\n",
    "print(\"Using Where for Join Condition- DEPT, ADD & EMP\")\n",
    "empDF.join(deptDF).where(empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
    "    .join(addDF).where(empDF[\"emp_id\"] == addDF[\"emp_id\"]) \\\n",
    "    .show()\n",
    "    \n",
    "#SQL\n",
    "print(\"SQL JOIN\")\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "addDF.createOrReplaceTempView(\"ADD\")\n",
    "\n",
    "spark.sql(\"select * from EMP e, DEPT d, ADD a \" + \\\n",
    "    \"where e.emp_dept_id == d.dept_id and e.emp_id == a.emp_id\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4f506",
   "metadata": {},
   "source": [
    "# TYPES of JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bba73d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "Emp DF\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "Dept DF\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "---------INNER JOIN------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 407:==========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "---------OUTER JOIN------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "---------FULL JOIN------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 417:>                                                        (0 + 4) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "---------FULL_OUTER JOIN------------\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "---------LEFT JOIN------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 427:============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "---------LEFT_OUTER JOIN------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 433:==============>                                          (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "---------RIGHT JOIN------------\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "---------RIGHT_OUTER JOIN------------\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "---------LEFT_SEMI JOIN------------\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "---------LEFT_ANTI JOIN------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 456:==============>                                          (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|2     |Rose    |1              |Smith            |\n",
      "|3     |Williams|1              |Smith            |\n",
      "|4     |Jones   |2              |Rose             |\n",
      "|5     |Brown   |2              |Rose             |\n",
      "|6     |Brown   |2              |Rose             |\n",
      "+------+--------+---------------+-----------------+\n",
      "\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "---------SQL JOIN------------\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "print(\"Emp DF\")\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "print(\"Dept DF\")\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "print(\"---------INNER JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "print(\"---------OUTER JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "print(\"---------FULL JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "print(\"---------FULL_OUTER JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
    "    .show(truncate=False)\n",
    "    \n",
    "print(\"---------LEFT JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "print(\"---------LEFT_OUTER JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "print(\"---------RIGHT JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "print(\"---------RIGHT_OUTER JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "print(\"---------LEFT_SEMI JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "print(\"---------LEFT_ANTI JOIN------------\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
    "   .show(truncate=False)\n",
    "   \n",
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
    "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
    "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
    "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
    "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "print(\"---------SQL JOIN------------\")\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef59d8",
   "metadata": {},
   "source": [
    "## Pivot and Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78c8539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "Fruits DF\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n",
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "Pivoted Fruits DF\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n",
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "Pivoted GroupBy Fruits DF\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n",
      "UnPivot\n",
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "|Orange |China  |4000 |\n",
      "|Beans  |China  |1500 |\n",
      "|Beans  |Mexico |2000 |\n",
      "|Banana |Canada |2000 |\n",
      "|Banana |China  |400  |\n",
      "|Carrots|Canada |2000 |\n",
      "|Carrots|China  |1200 |\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "print(\"Fruits DF\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "print(\"Pivoted Fruits DF\")\n",
    "pivotDF.show(truncate=False)\n",
    "\n",
    "pivotDF = df.groupBy(\"Product\",\"Country\") \\\n",
    "      .sum(\"Amount\") \\\n",
    "      .groupBy(\"Product\") \\\n",
    "      .pivot(\"Country\") \\\n",
    "      .sum(\"sum(Amount)\")\n",
    "pivotDF.printSchema()\n",
    "print(\"Pivoted GroupBy Fruits DF\")\n",
    "pivotDF.show(truncate=False)\n",
    "\n",
    "\"\"\" unpivot \"\"\"\n",
    "\n",
    "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
    "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
    "    .where(\"Total is not null\")\n",
    "print(\"UnPivot\")\n",
    "unPivotDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ced543",
   "metadata": {},
   "source": [
    "## fill & fillNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bf0c095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "RAW data\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|null               |PR   |30100     |\n",
      "|2  |704    |null    |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709    |null    |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|null               |TX   |null      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "fillna data\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "fillna subset population data\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "fillna '__' data\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|                   |   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|                   |   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "na.fill '__' data\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|                   |   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|                   |   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "filePath=\"rawdata/small_zipcode.csv\"\n",
    "df = spark.read.options(header='true', inferSchema='true') \\\n",
    "          .csv(filePath)\n",
    "\n",
    "df.printSchema()\n",
    "print(\"RAW data\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "print(\"fillna data\")\n",
    "df.fillna(value=0).show()\n",
    "print(\"fillna subset population data\")\n",
    "df.fillna(value=0,subset=[\"population\"]).show()\n",
    "df.na.fill(value=0).show()\n",
    "df.na.fill(value=0,subset=[\"population\"]).show()\n",
    "\n",
    "print(\"fillna '__' data\")\n",
    "df.fillna(value=\"\").show()\n",
    "print(\"na.fill '__' data\")\n",
    "df.na.fill(value=\"\").show()\n",
    "\n",
    "df.fillna(\"unknown\",[\"city\"]) \\\n",
    "    .fillna(\"\",[\"type\"]).show()\n",
    "\n",
    "df.fillna({\"city\": \"unknown\", \"type\": \"\"}) \\\n",
    "    .show()\n",
    "\n",
    "df.na.fill(\"unknown\",[\"city\"]) \\\n",
    "    .na.fill(\"\",[\"type\"]).show()\n",
    "\n",
    "df.na.fill({\"city\": \"unknown\", \"type\": \"\"}) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52202e3",
   "metadata": {},
   "source": [
    "## sample example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd85b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 45|\n",
      "| 61|\n",
      "| 65|\n",
      "| 66|\n",
      "| 67|\n",
      "| 77|\n",
      "| 80|\n",
      "| 84|\n",
      "| 94|\n",
      "|100|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 45|\n",
      "| 61|\n",
      "| 65|\n",
      "| 66|\n",
      "| 67|\n",
      "| 77|\n",
      "| 80|\n",
      "| 84|\n",
      "| 94|\n",
      "|100|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(start=1, end=101)\n",
    "df1 = df.sample(fraction=0.1, seed=123).show()\n",
    "df2 = df.sample(fraction=0.1, seed=123).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a01b9",
   "metadata": {},
   "source": [
    "## collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "695958b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "----------------------------DEPT data\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "----------------------------DEPT data collect\n",
      "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n",
      "----------------------------DEPT data collect2\n",
      "[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\n",
      "----------------------------DEPT datacollect iterate over each row\n",
      "Finance,10\n",
      "Marketing,20\n",
      "Sales,30\n",
      "IT,40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "print(\"----------------------------DEPT data\")\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "dataCollect = deptDF.collect()\n",
    "\n",
    "print(\"----------------------------DEPT data collect\")\n",
    "print(dataCollect)\n",
    "dataCollect2 = deptDF.select(\"dept_name\").collect()\n",
    "print(\"----------------------------DEPT data collect2\")\n",
    "print(dataCollect2)\n",
    "\n",
    "print(\"----------------------------DEPT datacollect iterate over each row\")\n",
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" +str(row['dept_id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb50d55",
   "metadata": {},
   "source": [
    "## transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb01af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      "\n",
      "Raw DATA\n",
      "+----------+----+--------+\n",
      "|CourseName|fee |discount|\n",
      "+----------+----+--------+\n",
      "|Java      |4000|5       |\n",
      "|Python    |4600|10      |\n",
      "|Scala     |4100|15      |\n",
      "|Scala     |4500|15      |\n",
      "|PHP       |3000|20      |\n",
      "+----------+----+--------+\n",
      "\n",
      "Customized Transformations---> UpperCase, ReducePrice and Discount\n",
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName| fee|discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|      JAVA|4000|       5|   3000|        2850.0|\n",
      "|    PYTHON|4600|      10|   3600|        3240.0|\n",
      "|     SCALA|4100|      15|   3100|        2635.0|\n",
      "|     SCALA|4500|      15|   3500|        2975.0|\n",
      "|       PHP|3000|      20|   2000|        1600.0|\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Languages1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Languages2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "Raw data for SQL transform\n",
      "+----------------+------------------+---------------+\n",
      "|            Name|        Languages1|     Languages2|\n",
      "+----------------+------------------+---------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
      "+----------------+------------------+---------------+\n",
      "\n",
      "using transform() SQL lambda function\n",
      "+------------------+\n",
      "|        languages1|\n",
      "+------------------+\n",
      "|[JAVA, SCALA, C++]|\n",
      "|[SPARK, JAVA, C++]|\n",
      "|      [CSHARP, VB]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "print('Raw DATA')\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Custom transformation 1\n",
    "from pyspark.sql.functions import upper\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
    "\n",
    "# Custom transformation 2\n",
    "def reduce_price(df,reduceBy):\n",
    "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
    "\n",
    "# Custom transformation 3\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discounted_fee\",  \\\n",
    "             df.new_fee - (df.new_fee * df.discount) / 100)\n",
    "\n",
    "# transform() usage\n",
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "        .transform(reduce_price,1000) \\\n",
    "        .transform(apply_discount) \n",
    "\n",
    "print(\"Customized Transformations---> UpperCase, ReducePrice and Discount\")\n",
    "df2.show()\n",
    "\n",
    "# Create DataFrame with Array\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "]\n",
    "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
    "df.printSchema()\n",
    "print('Raw data for SQL transform')\n",
    "df.show()\n",
    "\n",
    "# using transform() SQL function\n",
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.functions import transform\n",
    "print(\"using transform() SQL lambda function\")\n",
    "df.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba89e90",
   "metadata": {},
   "source": [
    "# date functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc954a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('SparkByExamples.com') \\\n",
    "            .getOrCreate()\n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "706e48f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_date function\n",
      "+----------+----------+\n",
      "|     input|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n",
      "date_diff function\n",
      "+----------+--------+\n",
      "|     input|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|    1249|\n",
      "|2019-03-01|    1586|\n",
      "|2021-03-01|     855|\n",
      "+----------+--------+\n",
      "\n",
      "date month between function\n",
      "+----------+--------------+\n",
      "|     input|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   41.09677419|\n",
      "|2019-03-01|   52.09677419|\n",
      "|2021-03-01|   28.09677419|\n",
      "+----------+--------------+\n",
      "\n",
      "date trunc function\n",
      "+----------+-----------+----------+-----------+\n",
      "|     input|Month_Trunc|Month_Year|Month_Trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n",
      "|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n",
      "|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n",
      "add_months , date_add, date_sub function\n",
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n",
      "year, month, month, next_day, weekofyear function\n",
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n",
      "new date data\n",
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n",
      "current_timestamp function\n",
      "+--------------------------+\n",
      "|current_timestamp         |\n",
      "+--------------------------+\n",
      "|2023-07-04 21:47:19.828384|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "to_timestamp function\n",
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n",
      "hour, Minute and second\n",
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n",
      "THE END\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#to_date()\n",
    "print(\"to_date function\")\n",
    "df.select(col(\"input\"), \n",
    "    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\") \n",
    "  ).show()\n",
    "\n",
    "#datediff()\n",
    "print(\"date_diff function\")\n",
    "df.select(col(\"input\"), \n",
    "    datediff(current_date(),col(\"input\")).alias(\"datediff\")  \n",
    "  ).show()\n",
    "\n",
    "print(\"date month between function\")\n",
    "#months_between()\n",
    "df.select(col(\"input\"), \n",
    "    months_between(current_date(),col(\"input\")).alias(\"months_between\")  \n",
    "  ).show()\n",
    "\n",
    "print(\"date trunc function\")\n",
    "\n",
    "#trunc()\n",
    "df.select(col(\"input\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n",
    "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
    "   ).show()\n",
    "\n",
    "print(\"add_months , date_add, date_sub function\")\n",
    "\n",
    "#add_months() , date_add(), date_sub()\n",
    "df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\") \n",
    "  ).show()\n",
    "\n",
    "\n",
    "print(\"year, month, month, next_day, weekofyear function\")\n",
    "\n",
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
    "  ).show()\n",
    "\n",
    "\n",
    "print(\"new date data\")\n",
    "\n",
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)\n",
    "\n",
    "print(\"current_timestamp function\")\n",
    "\n",
    "#current_timestamp()\n",
    "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  ).show(1,truncate=False)\n",
    "\n",
    "print(\"to_timestamp function\")\n",
    "\n",
    "#to_timestamp()\n",
    "df2.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).show(truncate=False)\n",
    "\n",
    "print(\"hour, Minute and second\")\n",
    "\n",
    "#hour, minute,second\n",
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)\n",
    "\n",
    "\n",
    "print(\"THE END\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccb451",
   "metadata": {},
   "source": [
    "## User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ecc120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data\n",
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n",
      "Using UDF with select\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------------+\n",
      "|Seqno|        Name|to_upper(Name)|\n",
      "+-----+------------+--------------+\n",
      "|    1|  john jones|    JOHN JONES|\n",
      "|    2|tracey smith|  TRACEY SMITH|\n",
      "|    3| amy sanders|   AMY SANDERS|\n",
      "+-----+------------+--------------+\n",
      "\n",
      "Using UDF with withColumn\n",
      "+-----+------------+------------+\n",
      "|Seqno|        Name|   upper_col|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "print(\"Raw Data\")\n",
    "df.show()\n",
    "\n",
    "# imports\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "# create pandas_udf\n",
    "@pandas_udf(StringType())\n",
    "def to_upper(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "# Using UDF with select()\n",
    "print(\"Using UDF with select\")\n",
    "df.select(\"Seqno\",\"Name\",to_upper(\"Name\")).show()\n",
    "\n",
    "# Using UDF with withColumn()\n",
    "print(\"Using UDF with withColumn\")\n",
    "df.withColumn(\"upper_col\",to_upper(\"Name\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f538d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fa9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3779c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbbfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84096126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc9676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedb017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
